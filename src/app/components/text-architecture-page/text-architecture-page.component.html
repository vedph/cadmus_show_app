<div>
  <h1>Text Architecture</h1>
  <div>
    <div id="banner"></div>
  </div>
  <article>
    <h2>Text in Cadmus</h2>
    <aside class="pquote">
      <blockquote>Text as a part</blockquote>
    </aside>
    <p>
      As explained about
      <a routerLink="/docs/data-architecture">data architecture</a>, in Cadmus
      data are "objects" (named parts) tossed into "boxes" (named items). Each
      item has a composite data model, built of self-contained, semantically
      coherent sub-models, which represent its parts.
    </p>
    <p>
      Items can be anything you may want to represent, from physical objects to
      abstract entities, also including text with all its metadata. An essential
      feature of this architecture is that the text itself is no more the
      titanic structure condemned to bear the whole world of data, like Atlas;
      text here is just a datum like any other, i.e. a part, one of the objects
      tossed in a box.
    </p>
    <aside class="pquote">
      <blockquote>Single-structure limitations</blockquote>
    </aside>
    <p>
      In fact, modelling text as the unique structure on top of which all data
      are laid out is the main cause of issues for some types of complex digital
      editions. If you think to an XML tree, which is the usual technology in
      this context, this tree is the only structure. All metatextual
      annotations, and even non-textual data, must be somehow attached to it.
      This usually is perfectly fine; but there are cases where we are going to
      deal with a set of meta-textual or non-textual data which is much bigger
      than the text itself, and may require highly structured, independent
      models for each of its entities. This often happens either because of the
      very nature of the text, or because we're trying to stuff more and more
      data in it.
    </p>
    <aside class="pquote">
      <blockquote>Intrinsic complexity</blockquote>
    </aside>
    <p>
      A typical case of the former type is represented by an inscription. Even
      the shortest one may require a lot of highly specialized metadata, related
      to very different domains. Take for instance the inscription shown here;
      its text runs for a dozen of words:
    </p>
    <cite style="text-align: center">
      <p>
        Decentius XP in pacem<br />
        qui bixit • annis • V • menses • VI • depositus<br />
        die • XIIII • Kal • Apriles •
      </p>
    </cite>
  </article>
  <div>
    <img
      style="display: block; margin: auto"
      src="/assets/img/decentius.jpg"
      alt="Inscription"
    />
  </div>
  <article>
    <h2>A Lot of Data on Text</h2>
    <aside class="pquote">
      <blockquote>Metadata from different domains</blockquote>
    </aside>
    <p>
      Now, here we could add full prosopographic data to the name
      <em>Decentius</em>; a paleographic description for the chrismon; explain
      in a comment that <em>in pacem</em> implies a verb like
      <em>requiescit</em>; provide a linguistic annotation about the
      <em>BIXIT</em> (=<em>vixit</em>) orthography; again, add Decentius' age
      (<em>annis V menses VI</em>) to his prosopographic data; convert the Roman
      date (die XIII Kal. Apriles = March, 19); resolve the abbreviation (<em
        >KAL</em
      >
      = <em>Kalendas</em>); etc.
    </p>
    <aside class="pquote">
      <blockquote>Extra-textual data</blockquote>
    </aside>
    <p>
      All these are meta-textual data, i.e. data which are connected to a
      specific portion of the text. But we also have extra-textual data, like
      archaeological context, excavation data, physical description, etc. So,
      even with just a few words, our annotation possibilities are endless.
    </p>
    <aside class="pquote">
      <blockquote>Extrinsic complexity</blockquote>
    </aside>
    <p>
      Also, besides intrinsically complex documents, added complexity eventually
      comes from the systematic application of analysis tools on texts. This
      happens more and more frequently nowadays, for instance with POS tagging
      in morphology; tree tagging in syntax; automatic metric analysis; etc. The
      results of such tools would be encoded in multiple structures, often not
      compatible among them. This would produce a more and more complex schema,
      which still must be fitted in the same, unique structure; eventually
      hitting the well-known XML overlap barrier.
    </p>
    <aside class="pquote">
      <blockquote>The risk of polluting schemas</blockquote>
    </aside>
    <p>
      Overlap is the ultimate barrier for XML, and it potentially comes up a lot
      of times. Yet, in most cases it's very easy to overcome it by nesting a
      tag inside another one, or by adding attributes to a single tag including
      both annotations.
    </p>
    <p>
      For instance, say a couple of words like QVE VIXIT occur in an inscription
      with a graphical connection (ligature) between E and V. Imagine that we
      have an XML tag to encode the historical orthography E for our standard AE
      (QV[AE]); and another tag to encode the ligature between E and V (QVA[E
      V]IXIT). It's easy to see that both these tags would overlap on letter E.
      Overcoming this is trivial, but at the cost of mixing different semantic
      domains (there is no connection between a ligature and the
      monophthongization of AE).
    </p>
    <p>
      Further, it's equally easy to imagine how things can get complicated when
      we have a big number of highly granular annotations, ranging from a single
      character to whole sentences, each with its own semantic domain, which
      would require its own modelling. Even if in a single domain, models can
      become very complex: just think about critical apparatus, and the various
      strategies to encode it in XML-TEI (Location-Referenced, Parallel
      Segmentation, Double End Point Attached), until we must resort to
      duplicate the XML structure, by using two distinct documents with standoff
      notation (which is very difficult to do by hand). You can learn more about
      these issues in
      <a
        href="https://www.youtube.com/watch?v=lYykjz26TCg&t=838"
        target="_blank"
        >my presentation</a
      >.
    </p>
    <p>
      So, in a single structure like that of an XML document, bearing both text
      and meta-textual annotations, every tag must be entangled in the existing
      mosaic of tags. Just like attaching two balls on the same branch of a
      tree, we have mutual constraints: adding new data is constrained (and
      shaped) by existing data; and in turn it affects them, by changing the
      tree's shape. Further, in some cases, data might be too complex, or too
      big, to fit the tree. Yet, in this scenario data cannot live outside it.
    </p>
    <aside class="pquote">
      <blockquote>An Occam's razor</blockquote>
    </aside>
    <p>
      The Cadmus approach here comes with an Occam's razor to simply drop our
      tree: it rather puts the balls (and any other object) in some boxes. We no
      more have a single tree holding all the data; so data is free to be
      modelled according to its own domain. Also, data has no necessary relation
      with text: it can be anything. And finally, new data, and new data models,
      can be thrown into a box at any moment, without affecting existing data,
      nor being constrained by it.
    </p>
    <aside class="pquote">
      <blockquote>The added benefits of switching infrastructure</blockquote>
    </aside>
    <p>
      All data in Cadmus are just objects in boxes, and text is no exception.
      Plus, this architecture implies that we no more rely on the file system to
      store data; we rather use a true database, and provide a user-friendly
      graphical UI to edit them. Of course, this change affects the whole
      content creation process, which becomes a totally different user
      experience: no more tagging a text, which requires technical skills and
      sometimes hacks to encode things the way we want. Rather, users enter a
      web application, browse and edit data in rich UIs, while getting all the
      features expected from an ordinary editing system (concurrent editing,
      centralized data store, robustness, real-time validation and search
      capabilities, user authentication and different authorization levels,
      continuous backup, auditing, full editing history, etc.).
    </p>
    <h2>Parts and Fragments: Layered Text</h2>
    <aside class="pquote">
      <blockquote>Text as parts</blockquote>
    </aside>
    <p>
      So, text is just a datum, a part in Cadmus lingo. This usually implies
      that longer texts get chunked into smaller, more manageable pieces, just
      like in a TEI document we divide it into div sections. Of course this
      happens adopting a semantic criterion, so that we can deal with relatively
      self-contained and meaningful chunks.
    </p>
    <p>
      This is usually all what is required for a text part; entering it is as
      easy as typing or pasting in a simple textbox. We thus end up with a
      uniform architecture, with objects of any types distributed in different
      boxes, whatever their relationship with text: an object can indifferently
      represent text, or non-textual data.
    </p>
    <aside class="pquote">
      <blockquote>Meta-textual data as parts</blockquote>
    </aside>
    <p>
      The same holds with meta-textual data, i.e. those data which are attached
      to a specific portion of a text. Typically, this is what corresponds to
      tags in an XML document. It should now come as no surprise that in Cadmus
      meta-textual data are no different from any other datum: they are just
      parts. Their only distinctive is that their model always consists of a
      collection of sub-objects, whose model is designed to represent data from
      a specialized domain. These sub-objects are named
      <strong>fragments</strong>. Each fragment is linked to a specific portion
      of the reference text, which as we have seen is separately stored in
      another part.
    </p>
    <aside class="pquote">
      <blockquote>Text layers</blockquote>
    </aside>
    <p>
      There is no limit to the complexity of a fragment's model. Each set of
      fragments sharing the same model belong to the same meta-textual part,
      which can thus be seen as a <strong>layer</strong> on top of the reference
      text part. Just like in GIS maps we stack specialized information in
      several different layers, each devoted to a specific domain, here we stack
      meta-textual layers on top of a text.
    </p>
    <p>
      Going back to the above inscription sample, here we would end up with 1
      item, representing it, with 7 parts:
    </p>
    <p>(1) 1 <em>text</em> part for the inscription's text.</p>
    <p>
      (2) 1 <em>prosopographic layer</em> part to add specialized annotations
      about all the persons cited in a text.
    </p>
    <p>
      (3) 1 <em>paleographic layer</em> part to describe all the relevant
      paleographic features, like the chrismon or the punctuation signs.
    </p>
    <p>
      (4) 1 <em>comment layer</em> part to contain any general-purpose comments
      we want to attach to specific portions of the text. For instance, here we
      might have 2 fragments: one about the implied word "requiescit" in the
      first sentence, and another about the ablative case used in "annis", which
      happens to be rare with respect to the more common accusative ("annos");
      eventually, this comment would include also some references, like a
      reference to Sandys 1927 for this syntactic annotation.
    </p>
    <p>
      Also, different comments might be tagged in different ways: for instance,
      the "requiescit" comment would be targeted to unexperienced users, whereas
      the syntactic comment about "annis" would be useful to a more learned
      audience. Further, each comment might also be tagged with reference to an
      external taxonomy, maybe related to relevant linguistic phenomena,
      relevant graphic features, or whatever else deserves special attention in
      our edition.
    </p>
    <p>
      (5) 1 <em>orthography layer</em> part to contain all the standard
      orthography forms linked to the corresponding historical forms found in
      the text. For instance, here we would have an annotation telling us that
      "bixit" corresponds to "vixit". Of course, this is relevant for the
      history of the language, so our orthography model is not limited to
      providing the standard orthography, but can also be extended to encode the
      details of each linguistic phenomenon underlying this orthography (see the
      orthography fragment model in the
      <a routerLink="/models/shop">shop</a> for more).
    </p>
    <p>
      In turn, these phenomena can be connected to taxonomies, which can also
      have a deep hierarchical structure. For instance, B for V in "vixit" might
      be tagged as spirantization of plosives; in turn, this phenomenon might be
      under the wider category of linguistic phenomena related to plosives; in
      turn, plosives would be under consonants; and consonants under phonology.
    </p>
    <p>
      It's easy to imagine the potential of this kind of annotations across the
      whole spectrum of relevant linguistic phenomena, for all the texts of a
      corpus, allowing highly granular and specialized searches. And of course,
      the same holds for any other layer, even beyond the intentions of their
      original creators.
    </p>
    <p>
      For instance, an epigraphist interested in paleographic features might
      take care of annotating each punctuation sign; and this could be used well
      beyond the original intent, maybe by a linguist or a philologist, who are
      looking for all the factors contributing to the definition of a list of
      appositive words. As a matter of fact, ancient punctuation here is often
      of help, as far as it does not occur between an appositive and its "host"
      word. This is the case of "qui bixit" in our inscription, where the
      prepositive status of the monosyllabic pronoun "qui" does not trigger a
      punctuation sign which is used elsewhere in this inscription. That's just
      a trivial sample, but it should be enough to emphasize the potential of a
      highly refined annotation on top of several layers, each with its own
      model, defined only whithin the boundaries of its conceptual domain, and
      not affected by limitations imposed by any specific storage technology.
    </p>
    <p>
      (6) 1 <em>chronology</em> layer part to contain all the chronological data
      which can be attached to specific text portions. Here we just have a date,
      but this layer might be handy for whatever textual reference which can be
      dated, like a priesthood, a consulate, a battle, etc.
    </p>
    <p>
      (7) 1 <em>abbreviations</em> layer part to resolve all the abbreviated
      forms in the original text, like KAL for Kalendas.
    </p>
    <aside class="pquote">
      <blockquote>Scalability</blockquote>
    </aside>
    <p>
      Of course, this is far from exhausting the list of layers; for instance,
      another typical layer would be an apparatus, where we collect different
      types of annotations (proposed restorations, corrections, or expunctions;
      notes strictly related to the text constitution; etc.). There is no limit
      to the number of layers we can stack on top of a text; and whenever we add
      a new layer, this does not affect in any way the existing text nor the
      other layers. Compare this with a traditional XML-based text, where each
      new annotation implies modifying or even refactoring the tree structure to
      allow for it.
    </p>
    <p>
      As a final remark, we should also emphasize how this architecture allows
      not only adding new data, but also new data types (models), at any moment
      in the lifetime of the project. This is an open, dynamic data modelling
      architecture, where you can add as many new models as you want, without
      having to care about side effects on existing data. Each model is
      independent, and designed in the context of its conceptual domain; it's
      somehow free from the cage of a specific technology and physical modelling
      constraints. We can even think of an edition as the foundation for any
      type of future expansions, in whatever direction. For instance, we could
      start with a basic collection of raw inscription texts, and later decide
      to progressively add new layers for linguistics, history, prospography,
      paleography, metrics, or any other specialization we can think of. We thus
      embrace the dynamic, Heraclitean flux of the digital edition, which
      becomes a strength rather than an issue.
    </p>
    <aside class="pquote">
      <blockquote>A models shop</blockquote>
    </aside>
    <p>
      In this context, the idea of a
      <a routerLink="/models/shop">"models shop"</a> comes in naturally; as
      projects start defining their own data models as parts, they are not only
      developing their own content, but also providing reusable models for other
      projects. For instance, once a project has defined a good and generic
      enough data model for critical apparatus, any other project requiring an
      apparatus will not have to reinvent the wheel. Rather, it will just import
      that model, and integrate it in its own architecture, which ultimately is
      as easy as tossing a new type of object in its boxes.
    </p>
    <aside class="pquote">
      <blockquote>Data reuse, UI reuse</blockquote>
    </aside>
    <p>
      A crucial feature of Cadmus here is that not only we are going to reuse
      the data model; we are also going to reuse its full-blown editing user
      interface. Just like we build our dynamic data models by composition
      (items consisting of parts), we build our content creation system by
      composition (each part or fragment having its own editor). This can have a
      huge impact on each project's economy, and also the benefit of promoting a
      virtuous circle, where standards can potentially emerge starting from the
      foundations laid out by several different projects working around the same
      concepts.
    </p>
  </article>
</div>
